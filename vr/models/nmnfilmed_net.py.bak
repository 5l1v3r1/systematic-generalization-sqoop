#!/usr/bin/env python3

# Copyright 2017-present, Facebook, Inc.
# All rights reserved.
#
# This source code is licensed under the license found in the
# LICENSE file in the root directory of this source tree.

import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.autograd import Variable
import torchvision.models

from vr.models.layers import init_modules, ResidualBlock, GlobalAveragePool, Flatten
from vr.models.layers import build_classifier, build_stem, ConcatBlock
import vr.programs

from vr.models.tfilmed_net import TfilmedResBlock, ConCatTfilmBlock, coord_map

class NMNFiLM(nn.Module):
  def __init__(self, vocab, feature_dim=(1024, 14, 14),
               stem_num_layers=2,
               stem_batchnorm=False,
               stem_kernel_size=3,
               stem_subsample_layers=None,
               stem_stride=1,
               stem_padding=None,
               num_modules=4,

               module_num_layers=1,
               module_dim=128,
               module_residual=True,
               module_intermediate_batchnorm=False,
               module_batchnorm=False,
               module_batchnorm_affine=False,
               module_dropout=0,
               module_input_proj=1,
               module_kernel_size=3,
               classifier_proj_dim=512,
               classifier_downsample='maxpool2',
               classifier_fc_layers=(1024,),
               classifier_batchnorm=False,
               classifier_dropout=0,
               condition_method='bn-film',
               condition_pattern=[],
               use_gamma=True,
               use_beta=True,
               use_coords=1,
               debug_every=float('inf'),
               print_verbose_every=float('inf'),
               verbose=True):
    super(NMNFiLM, self).__init__()
    
    num_answers = len(vocab['answer_idx_to_token'])

    self.stem_times = []
    self.module_times = []
    self.classifier_times = []
    self.timing = False

    self.num_modules = num_modules

    self.module_num_layers = module_num_layers
    self.module_batchnorm = module_batchnorm
    self.module_dim = module_dim
    self.condition_method = condition_method
    self.use_gamma = use_gamma
    self.use_beta = use_beta
    self.use_coords_freq = use_coords
    self.debug_every = debug_every
    self.print_verbose_every = print_verbose_every

    # Initialize helper variables
    self.stem_use_coords = (stem_stride == 1) and (self.use_coords_freq > 0)
    self.condition_pattern = condition_pattern

    self.extra_channel_freq = self.use_coords_freq
    #self.block = FiLMedResBlock
    self.num_cond_maps = 2 * self.module_dim if self.condition_method == 'concat' else 0
    
    self.fwd_count = 0
    
    self.num_extra_channels = 2 if self.use_coords_freq > 0 else 0
    if self.debug_every <= -1:
      self.print_verbose_every = 1
    
    module_H = feature_dim[1] // (stem_stride ** stem_num_layers)  # Rough calc: work for main cases
    module_W = feature_dim[2] // (stem_stride ** stem_num_layers)  # Rough calc: work for main cases
    for _ in stem_subsample_layers:
      module_H //= 2
      module_W //= 2
    
    self.stem_coords = coord_map((feature_dim[1], feature_dim[2]))
    self.coords = coord_map((module_H, module_W))
    self.default_weight = Variable(torch.ones(1, 1, self.module_dim)).type(torch.cuda.FloatTensor)
    self.default_bias = Variable(torch.zeros(1, 1, self.module_dim)).type(torch.cuda.FloatTensor)

    # Initialize stem
    stem_feature_dim = feature_dim[0] + self.stem_use_coords * self.num_extra_channels
    self.stem = build_stem(stem_feature_dim, module_dim,
                           num_layers=stem_num_layers, with_batchnorm=stem_batchnorm,
                           kernel_size=stem_kernel_size, stride=stem_stride, padding=stem_padding,
                           subsample_layers=stem_subsample_layers)

    if verbose:
      print('Here is my stem:')
      print(self.stem)

    self.function_modules = {}
    self.function_modules_num_inputs = {}
    self.vocab = vocab

    for fn_str in vocab['program_token_to_idx']:
      num_inputs = vocab['program_token_arity'][fn_str]
      self.function_modules_num_inputs[fn_str] = num_inputs
      with_cond = [self.condition_method != 'concat'] * (2*self.module_num_layers)
      if fn_str == 'scene' or num_inputs == 1:
        mod = TfilmedResBlock(module_dim, with_residual=module_residual,
                              with_intermediate_batchnorm=module_intermediate_batchnorm, with_batchnorm=module_batchnorm,
                              with_cond=with_cond,
                              dropout=module_dropout,
                              num_extra_channels=self.num_extra_channels,
                              extra_channel_freq=self.extra_channel_freq,
                              with_input_proj=module_input_proj,
                              num_cond_maps=self.num_cond_maps,
                              kernel_size=module_kernel_size,
                              batchnorm_affine=module_batchnorm_affine,
                              num_layers=self.module_num_layers,
                              condition_method=condition_method,
                              debug_every=self.debug_every)
      elif num_inputs == 2:
        mod = ConCatTfilmBlock(2, module_dim, with_residual=module_residual,
                               with_intermediate_batchnorm=module_intermediate_batchnorm, with_batchnorm=module_batchnorm,
                               with_cond=with_cond,
                               dropout=module_dropout,
                               num_extra_channels=self.num_extra_channels,
                               extra_channel_freq=self.extra_channel_freq,
                               with_input_proj=module_input_proj,
                               num_cond_maps=self.num_cond_maps,
                               kernel_size=module_kernel_size,
                               batchnorm_affine=module_batchnorm_affine,
                               num_layers=self.module_num_layers,
                               condition_method=condition_method,
                               debug_every=self.debug_every)
      self.add_module(fn_str, mod)
      self.function_modules[fn_str] = mod
    
    self.save_module_outputs = False
    
    self.classifier = build_classifier(module_dim + self.num_extra_channels, module_H, module_W,
                                       num_answers, classifier_fc_layers, classifier_proj_dim,
                                       classifier_downsample, with_batchnorm=classifier_batchnorm,
                                       dropout=classifier_dropout)
    
    if verbose:
      print('Here is my classifier:')
      print(self.classifier)
    
    init_modules(self.modules())

  def expand_answer_vocab(self, answer_to_idx, std=0.01, init_b=-50):
    # TODO: This is really gross, dipping into private internals of Sequential
    final_linear_key = str(len(self.classifier._modules) - 1)
    final_linear = self.classifier._modules[final_linear_key]

    old_weight = final_linear.weight.data
    old_bias = final_linear.bias.data
    old_N, D = old_weight.size()
    new_N = 1 + max(answer_to_idx.values())
    new_weight = old_weight.new(new_N, D).normal_().mul_(std)
    new_bias = old_bias.new(new_N).fill_(init_b)
    new_weight[:old_N].copy_(old_weight)
    new_bias[:old_N].copy_(old_bias)

    final_linear.weight.data = new_weight
    final_linear.bias.data = new_bias

  def _forward_modules_ints_helper(self, feats, program, i, j):
    used_fn_j = True
    if j < program.size(1):
      fn_idx = program.data[i, j]
      fn_str = self.vocab['program_idx_to_token'][fn_idx]
    else:
      used_fn_j = False
      fn_str = 'scene'
    if fn_str == '<NULL>':
      used_fn_j = False
      fn_str = 'scene'
    elif fn_str == '<START>':
      used_fn_j = False
      return self._forward_modules_ints_helper(feats, program, i, j + 1)
    if used_fn_j:
      self.used_fns[i, j] = 1
    j += 1
    module = self.function_modules[fn_str]
    if fn_str == 'scene':
      module_inputs = [feats[i:i+1]]
    else:
      num_inputs = self.function_modules_num_inputs[fn_str]
      module_inputs = []
      while len(module_inputs) < num_inputs:
        cur_input, j = self._forward_modules_ints_helper(feats, program, i, j)
        module_inputs.append(cur_input)
    module_output = module(*module_inputs)
    return module_output, j

  def _forward_modules_ints(self, feats, program):
    """
    feats: FloatTensor of shape (N, C, H, W) giving features for each image
    program: LongTensor of shape (N, L) giving a prefix-encoded program for
      each image.
    """
    N = feats.size(0)
    final_module_outputs = []
    self.used_fns = torch.Tensor(program.size()).fill_(0)
    for i in range(N):
      cur_output, _ = self._forward_modules_ints_helper(feats, program, i, 0)
      final_module_outputs.append(cur_output)
    self.used_fns = self.used_fns.type_as(program.data).float()
    final_module_outputs = torch.cat(final_module_outputs, 0)
    return final_module_outputs

  def forward(self, x, program):
    N = x.size(0)
    assert N == len(program)

    feats = self.stem(x)

    if type(program) is list or type(program) is tuple:
      final_module_outputs = self._forward_modules_json(feats, program)
    elif type(program) is Variable and program.dim() == 2:
      final_module_outputs = self._forward_modules_ints(feats, program)
    elif torch.is_tensor(program) and program.dim() == 3:
      final_module_outputs = self._forward_modules_probs(feats, program)
    else:
      raise ValueError('Unrecognized program format')

    # After running modules for each input, concatenat the outputs from the
    # final module and run the classifier.
    out = self.classifier(final_module_outputs)
    return out